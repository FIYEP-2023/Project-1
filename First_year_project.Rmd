---
title: "First Year Project: Extinction prediciton"
output: pdf_document
date: "2023-02-07"
---

```{r setup, include=FALSE, results=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE, results=FALSE, message=FALSE}
if (!require("pacman")) install.packages("pacman")
if (!require("ggplot2")) install.packages("ggplot2")
if (!require("ggpmisc")) install.packages("ggpmisc")
if (!require("ggpubr")) install.packages("ggpubr")
if (!require("ggsignif")) install.packages("ggsignif")
library("ggplot2")
library("ggpmisc")
library("ggpubr")
library("ggsignif")
library(tibble)
library(ggplot2)
library(broom)
```

We start by importing the dataset directly from the csv file and saving it to the data variable:

```{r}
head(data <- read.csv('Factors Affecting Extinction.csv', header=T))
```

# (1) Initial Plotting

There are four different combinations of Size and Status, LR, LM, SR and SM. If we want to find the correlation between *extinction time* as a function of *pairs*, we can make a regression line with *pairs* as the predictor value and *extinction time* as the predicted value.

```{r}
ggplot(data, aes(x = Pairs, y = Time)) +
  geom_point() +
  facet_grid(Size ~ Status) +
  theme(legend.position = "top") +
  geom_smooth(method = "lm", formula = y ~ x) +
  
  stat_poly_eq(formula = y ~ x,
  aes(label = paste(after_stat(eq.label), after_stat(rr.label), sep = "~~~")),
  parse = TRUE) +
  
  labs(title = "Raw data") +
  theme_bw()
```

# (2) Residual plot of raw (not transformed) data
# This is the ONLY correct residual plot

```{r}
model <- lm(Time ~ Pairs, data = data)
y_hat <- predict(model, newdata = data)

ggplot(data,
  mapping = aes(x = y_hat,
                y = resid(lm(Time ~ Pairs, data = data)))) +
  geom_point() +
  facet_grid(Size ~ Status) +
  geom_hline(yintercept = 0, color = "red") +
  xlab("Predicted value") +
  ylab("Standarized Residuals") +
  labs(title = "Standardized Residual Plot (raw data)") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(legend.position = "bottom")
```

# (3) Transformations of time to log_time, sqrt_time, inverse_time
We now try different transformations on the data to see if we can get a better fitted regression line. We try 3 different transformation: log("time"), sqrt("time") and 1/("time").  

```{r}
data$log_time <- log2(data$Time)
data$sqrt_time <- sqrt(data$Time)
data$inverse_time <- 1/data$Time
head(data)
```
\newpage
Plotting of log_time:

```{r echo=FALSE, results=FALSE, message=FALSE}
transform_plots <- function(type, data, Y, transformation) {
  # type is one of "regres", "resid"
  data$Y = Y
  model <- lm(Time ~ Pairs, data = data)
  y_hat <- predict(model, newdata = data)
  result <- switch(
    type,
    "regres" = ggplot(
      data,
      aes(x = Pairs, y = Y)
    )
    + geom_point()
    + facet_grid(Size ~ Status) 
    + theme(legend.position = "top")
    + geom_smooth(
        method = "lm", 
        formula = y ~ x
      )
    + stat_poly_eq(
      formula = y ~ x,
      aes(
        label = paste(
          after_stat(
            eq.label
          ), 
          after_stat(
            rr.label
          ), 
          sep = "~~~"
        )
      ),
      parse = TRUE
    )
    + labs(
      title = sprintf("Raw data (%s)", transformation)
    )
    + ylab(
      sprintf("Time (%s)", transformation)
    )
    + theme_bw(),
    
    "resid" = ggplot(
      data,
      mapping = aes(
        x = y_hat,
        y = resid(
          lm(
              Y ~ Pairs, 
              data = data
            )
          )
        )
      )
    + geom_point()
    + facet_grid(Size ~ Status)
    + geom_hline(
        yintercept = 0, 
        color = "red"
      )
    + xlab("Predicted value")
    + ylab("Standarized Residuals")
    + labs(
      title = sprintf(
        "Standardized Residual Plot (raw data) (%s transformation)",
        transformation)
      )
    + theme_bw()
    + theme(plot.title = element_text(hjust = 0.5))
    + theme(legend.position = "bottom")

  )
  return(result)
}

for (transformation in c("log", "sqrt", "inverse")) {
  new_Y <- switch(
    transformation,
    "log" = log2(data$Time),
    "sqrt" = sqrt(data$Time),
    "inverse" = 1/data$Time
  )
  print(transform_plots("regres", data, new_Y, transformation))
  print(transform_plots("resid", data, new_Y, transformation))
}
```
As we can see, the downward trend of the points is no longer apparent. 
# (3)  Removing outliers and comparing  
We now proceed to remove outliers to see to what degree they affect the final result. We have opted to only graph and compare the logarithmic graph, as it has the best fit. We start by removing the maximum from the four plots:
```{r}
# Find the max and min values for each of the four plots
get_outliers <- function(data) {
  # Finds residual outliers and returns a vector in the form of (logical_max, logical_min) where logical_max is a vector of TRUE for the max value and FALSE everywhere else, and logical_min is the opposite.
  residuals = resid(lm(log2(Time) ~ Pairs, data = data))
  outliers <- c(
    max(residuals),
    min(residuals)
  )
  logicals <- data.frame(
    max = residuals == outliers[1],
    min = residuals == outliers[2]
  )
  return(logicals)
}
LR_mx <- get_outliers(data[
  data$Size == "L" &
  data$Status == "R",
])
LM_mx <- get_outliers(data[
  data$Size == "L" &
  data$Status == "M",
])
SR_mx <- get_outliers(data[
  data$Size == "S" &
  data$Status == "R",
])
SM_mx <- get_outliers(data[
  data$Size == "S" &
  data$Status == "M",
])

# Plot the graph by removing the four Max datapoints
# Get IDs of rows outlier rows
to_remove_ids = as.numeric(c(
  row.names(data[data$Size == "L" & data$Status == "R",][LR_mx$max,]),
  row.names(data[data$Size == "L" & data$Status == "M",][LM_mx$max,]),
  row.names(data[data$Size == "S" & data$Status == "R",][SR_mx$max,]),
  row.names(data[data$Size == "S" & data$Status == "M",][SM_mx$max,])
))

# Filter rows by removing those with the bad IDs
max_filt_data <- data[-to_remove_ids,]

# Plot time
new_Y <- log2(max_filt_data$Time)
print(transform_plots("regres", max_filt_data, new_Y, "log"))
```
Removing the max outliers from each plot significantly improved our $R^2$ value. If we also remove the min outliers, it will probably become an even better fit:
```{r}
# Plot the graph by removing the four Max datapoints
# Get IDs of rows outlier rows (max only)
to_remove_ids = as.numeric(c(
  row.names(data[data$Size == "L" & data$Status == "R",][LR_mx$min | LR_mx$max,]),
  row.names(data[data$Size == "L" & data$Status == "M",][LM_mx$min | LM_mx$max,]),
  row.names(data[data$Size == "S" & data$Status == "R",][SR_mx$min | SR_mx$max,]),
  row.names(data[data$Size == "S" & data$Status == "M",][SM_mx$min | SM_mx$max,])
))

# Filter rows by removing those with the bad IDs (max only)
max_filt_data <- data[-to_remove_ids,]

# Plot time
new_Y <- log2(max_filt_data$Time) # Need to redefine this because the row count has changed
print(transform_plots("regres", max_filt_data, new_Y, "log"))
```
Just removing the two extremes from the plots managed to increase one of our $R^2$ values from $0.53$ to $0.93$, an incredible improvement. Despite the fact that removing outliers improved the fit tremendously, it is a bad idea to do so. Data should only be removed if it has been either measured or written down incorrectly. Removing real data points is cherry-picking, and outliers can be a valuable insight into the data that has been collected. The mere fact that they can change line fittings this way is a testament to their influence - and thus their importance.  
q